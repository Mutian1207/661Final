{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhDUFBxt9xZg"
      },
      "source": [
        "# Implement and train a LSTM for sentiment analysis\n",
        "\n",
        "(General Hint on Lab 1/2: Trust whatever you see from the training and report it on PDF. IDMB is far from ideal as it's more like a real-world dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW6ymxu99xZk"
      },
      "source": [
        "## Step 0: set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spc_UH4B9xZl",
        "outputId": "42febfbd-c7c6-4317-cbeb-a48f03d43f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import os\n",
        "os.makedirs(\"resources\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ_7ECL4Z8HT"
      },
      "source": [
        "### Hyperparameters. Do not directly touch this to mess up settings.\n",
        "\n",
        "If you want to initalize new hyperparameter sets, use \"new_hparams = HyperParams()\" and change corresponding fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OxnFjs3f9xZn"
      },
      "outputs": [],
      "source": [
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        # Constance hyperparameters. They have been tested and don't need to be tuned.\n",
        "        self.PAD_INDEX = 0\n",
        "        self.UNK_INDEX = 1\n",
        "        self.PAD_TOKEN = '<pad>'\n",
        "        self.UNK_TOKEN = '<unk>'\n",
        "        self.STOP_WORDS = set(stopwords.words('english'))\n",
        "        self.MAX_LENGTH = 256\n",
        "        self.BATCH_SIZE = 96\n",
        "        self.EMBEDDING_DIM = 1\n",
        "        self.HIDDEN_DIM = 100\n",
        "        self.OUTPUT_DIM = 2\n",
        "        self.N_LAYERS = 1\n",
        "        self.DROPOUT_RATE = 0.0\n",
        "        self.LR = 0.01\n",
        "        self.N_EPOCHS = 5\n",
        "        self.WD = 0\n",
        "        self.OPTIM = \"sgd\"\n",
        "        self.BIDIRECTIONAL = False\n",
        "        self.SEED = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XODz_aDV9xZo"
      },
      "source": [
        "## Lab 1(a) Implement your own data loader function.  \n",
        "First, you need to read the data from the dataset file on the local disk. \n",
        "Then, split the dataset into three sets: train, validation and test by 7:1:2 ratio.\n",
        "Finally return x_train, x_valid, x_test, y_train, y_valid, y_test where x represents reviews and y represent labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AD7HSvM19xZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ab4682-ec0d-4294-f07c-76b892a7b43d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n"
          ]
        }
      ],
      "source": [
        "def load_imdb(base_csv:str = './IMDBDataset.csv'):\n",
        "    \"\"\"\n",
        "    Load the IMDB dataset\n",
        "    :param base_csv: the path of the dataset file.\n",
        "    :return: train, validation and test set.\n",
        "    \"\"\"\n",
        "    # Add your code here. \n",
        "    # print(\"hi\")\n",
        "    data = pd.read_csv(base_csv)\n",
        "    # print(data.get(\"sentiment\"))\n",
        "    x_train, x_test, y_train, y_test = train_test_split(data.get(\"review\"), data.get(\"sentiment\"), test_size = 0.2, random_state=1, shuffle = False)\n",
        "\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.125, random_state=1, shuffle = False) \n",
        "    \n",
        "    \n",
        "    \n",
        "    print(f'shape of train data is {x_train.shape}')\n",
        "    print(f'shape of test data is {x_test.shape}')\n",
        "    print(f'shape of valid data is {x_valid.shape}')\n",
        "    return x_train, x_valid, x_test, y_train, y_valid, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYVH6t--9xZq"
      },
      "source": [
        "## Lab 1(b): Implement your function to build a vocabulary based on the training corpus.\n",
        "Implement the build_vocab function to build a vocabulary based on the training corpus.\n",
        "You should first compute the frequency of all the words in the training corpus. Remove the words\n",
        "that are in the STOP_WORDS. Then filter the words by their frequency (≥ min_freq) and finally\n",
        "generate a corpus variable that contains a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sugI5VoJ9xZr"
      },
      "outputs": [],
      "source": [
        "def build_vocab(x_train:list, min_freq: int=5, hparams=None) -> dict:\n",
        "    \"\"\"\n",
        "    build a vocabulary based on the training corpus.\n",
        "    :param x_train:  List. The training corpus. Each sample in the list is a string of text.\n",
        "    :param min_freq: Int. The frequency threshold for selecting words.\n",
        "    :return: dictionary {word:index}\n",
        "    \"\"\"\n",
        "    # Add your code here. Your code should assign corpus with a list of words.\n",
        "    corpus = {}\n",
        "    for sentences in x_train:\n",
        "      sentence = sentences.split(\" \")\n",
        "      for word in sentence:\n",
        "        if word in hparams.STOP_WORDS:\n",
        "          continue\n",
        "        if word not in corpus:\n",
        "          corpus[word] = 1\n",
        "        else: \n",
        "          corpus[word] += 1\n",
        "\n",
        "    corpus_ = [word for word, freq in corpus.items() if freq >= min_freq]\n",
        "\n",
        "    # creating a dict\n",
        "    vocab = {w:i+2 for i, w in enumerate(corpus_)}\n",
        "    # print(vocab)\n",
        "\n",
        "    vocab[hparams.PAD_TOKEN] = hparams.PAD_INDEX\n",
        "    vocab[hparams.UNK_TOKEN] = hparams.UNK_INDEX\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca71G17F9xZt"
      },
      "source": [
        "## Lab 1(c): Implement your tokenize function. \n",
        "For each word, find its index in the vocabulary. \n",
        "Return a list of int that represents the indices of words in the example. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c6kj_qT69xZt"
      },
      "outputs": [],
      "source": [
        "def tokenize(vocab: dict, example: str)-> list:\n",
        "    \"\"\"\n",
        "    Tokenize the give example string into a list of token indices.\n",
        "    :param vocab: dict, the vocabulary.\n",
        "    :param example: a string of text.\n",
        "    :return: a list of token indices.\n",
        "    \"\"\"\n",
        "    # Your code here.\n",
        "    token_ind = []\n",
        "    example_arr = example.split(\" \")\n",
        "    for word in example_arr:\n",
        "      if word in vocab:\n",
        "        token_ind.append(vocab[word])\n",
        "    return token_ind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ntSo4k9xZu"
      },
      "source": [
        "## Lab 1 (d): Implement the __getitem__ function. Given an index i, you should return the i-th review and label. \n",
        "The review is originally a string. Please tokenize it into a sequence of token indices. \n",
        "Use the max_length parameter to truncate the sequence so that it contains at most max_length tokens. \n",
        "Convert the label string ('positive'/'negative') to a binary index. 'positive' is 1 and 'negative' is 0. \n",
        "Return a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2TDgA4p79xZu"
      },
      "outputs": [],
      "source": [
        "class IMDB(Dataset):\n",
        "    def __init__(self, x, y, vocab, max_length=256) -> None:\n",
        "        \"\"\"\n",
        "        :param x: list of reviews\n",
        "        :param y: list of labels\n",
        "        :param vocab: vocabulary dictionary {word:index}.\n",
        "        :param max_length: the maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"\n",
        "        Return the tokenized review and label by the given index.\n",
        "        :param idx: index of the sample.\n",
        "        :return: a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label.\n",
        "        \"\"\"\n",
        "        # Add your code here.\n",
        "        review = self.x.iloc[idx]\n",
        "        token = tokenize(self.vocab, review)\n",
        "        if len(token) >= self.max_length:\n",
        "          token_list = token[:self.max_length]\n",
        "        else:\n",
        "          token_list = token\n",
        "      \n",
        "        #get the label\n",
        "        label = self.y.iloc[idx]\n",
        "        tag = 1 if label == \"positive\" else 0\n",
        "\n",
        "        #create the dictionary\n",
        "        item_dict = {}\n",
        "        item_dict['ids'] = token_list\n",
        "        item_dict['length'] = len(token_list)\n",
        "        item_dict['label'] = tag\n",
        "\n",
        "        return item_dict\n",
        "        # pass\n",
        "    \n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "def collate(batch, pad_index):\n",
        "    batch_ids = [torch.LongTensor(i['ids']) for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_length = torch.Tensor([i['length'] for i in batch])\n",
        "    batch_label = torch.LongTensor([i['label'] for i in batch])\n",
        "    batch = {'ids': batch_ids, 'length': batch_length, 'label': batch_label}\n",
        "    return batch\n",
        "\n",
        "collate_fn = collate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zgSPYmf9xZv"
      },
      "source": [
        "## Lab 1 (e): Implement the LSTM model for sentiment analysis.\n",
        "Q(a): Implement the initialization function.\n",
        "Your task is to create the model by stacking several necessary layers including an embedding layer, a lstm cell, a linear layer, and a dropout layer.\n",
        "You can call functions from Pytorch's nn library. For example, nn.Embedding, nn.LSTM, nn.Linear.<br>\n",
        "Q(b): Implement the forward function.\n",
        "    Decide where to apply dropout. \n",
        "    The sequences in the batch have different lengths. Write/call a function to pad the sequences into the same length. \n",
        "    Apply a fully-connected (fc) layer to the output of the LSTM layer. \n",
        "    Return the output features which is of size [batch size, output dim]. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b9ofQ5R29xZv"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Embedding):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.orthogonal_(param)\n",
        "                \n",
        "class LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocab_size: int, \n",
        "        embedding_dim: int, \n",
        "        hidden_dim: int, \n",
        "        output_dim: int, \n",
        "        n_layers: int, \n",
        "        dropout_rate: float, \n",
        "        pad_index: int,\n",
        "        bidirectional: bool,\n",
        "        **kwargs):\n",
        "        \"\"\"\n",
        "        Create a LSTM model for classification.\n",
        "        :param vocab_size: size of the vocabulary\n",
        "        :param embedding_dim: dimension of embeddings\n",
        "        :param hidden_dim: dimension of hidden features\n",
        "        :param output_dim: dimension of the output layer which equals to the number of labels.\n",
        "        :param n_layers: number of layers.\n",
        "        :param dropout_rate: dropout rate.\n",
        "        :param pad_index: index of the padding token.we\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Add your code here. Initializing each layer by the given arguments.\n",
        "        \n",
        "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                            num_layers = n_layers, dropout = dropout_rate, bidirectional = bidirectional)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout()\n",
        "        # Weight initialization. DO NOT CHANGE!\n",
        "        if \"weight_init_fn\" not in kwargs:\n",
        "            self.apply(init_weights)\n",
        "        else:\n",
        "            self.apply(kwargs[\"weight_init_fn\"])\n",
        "\n",
        "\n",
        "    def forward(self, ids:torch.Tensor, length:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Feed the given token ids to the model.\n",
        "        :param ids: [batch size, seq len] batch of token ids.\n",
        "        :param length: [batch size] batch of length of the token ids.\n",
        "        :return: prediction of size [batch size, output dim].\n",
        "        \"\"\"\n",
        "        # Add your code here.\n",
        "        embeds = self.word_embedding(ids)\n",
        "        embed_padding = nn.utils.rnn.pack_padded_sequence(embeds, length, batch_first = True, enforce_sorted = False)\n",
        "        out, (h, c) = self.lstm(embed_padding)\n",
        "        lstm_out = h[-1]\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # out = self.sigmoid(out)\n",
        "        \n",
        "        # out = out.view(out.size(0), -1)\n",
        "        # out = out[:,-1]\n",
        "        \n",
        "        prediction = out\n",
        "        \n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "13Sdl7MV9xZv"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def train(dataloader, model, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
        "        ids = batch['ids'].to(device)\n",
        "        length = batch['length']\n",
        "        label = batch['label'].to(device)\n",
        "        prediction = model(ids, length)\n",
        "        loss = criterion(prediction, label)\n",
        "        accuracy = get_accuracy(prediction, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_accs.append(accuracy.item())\n",
        "        scheduler.step()\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids, length)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def get_accuracy(prediction, label):\n",
        "    batch_size, _ = prediction.shape\n",
        "    predicted_classes = prediction.argmax(dim=-1)\n",
        "    correct_predictions = predicted_classes.eq(label).sum()\n",
        "    accuracy = correct_predictions / batch_size\n",
        "    return accuracy\n",
        "\n",
        "def predict_sentiment(text, model, vocab, device):\n",
        "    tokens = tokenize(vocab, text)\n",
        "    ids = [vocab[t] if t in vocab else UNK_INDEX for t in tokens]\n",
        "    length = torch.LongTensor([len(ids)])\n",
        "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
        "    prediction = model(tensor, length).squeeze(dim=0)\n",
        "    probability = torch.softmax(prediction, dim=-1)\n",
        "    predicted_class = prediction.argmax(dim=-1).item()\n",
        "    predicted_probability = probability[predicted_class].item()\n",
        "    return predicted_class, predicted_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEQTQ1xqZ8HW"
      },
      "source": [
        "### Lab 1 (g) Implement GRU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lix_ifTxZ8HX"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocab_size: int, \n",
        "        embedding_dim: int, \n",
        "        hidden_dim: int, \n",
        "        output_dim: int, \n",
        "        n_layers: int, \n",
        "        dropout_rate: float, \n",
        "        pad_index: int,\n",
        "        bidirectional: bool,\n",
        "        **kwargs):\n",
        "        \"\"\"\n",
        "        Create a LSTM model for classification.\n",
        "        :param vocab_size: size of the vocabulary\n",
        "        :param embedding_dim: dimension of embeddings\n",
        "        :param hidden_dim: dimension of hidden features\n",
        "        :param output_dim: dimension of the output layer which equals to the number of labels.\n",
        "        :param n_layers: number of layers.\n",
        "        :param dropout_rate: dropout rate.\n",
        "        :param pad_index: index of the padding token.we\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Add your code here. Initializing each layer by the given arguments.\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim,\n",
        "                            num_layers = n_layers, dropout = dropout_rate, bidirectional = bidirectional)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        # Weight Initialization. DO NOT CHANGE!\n",
        "        if \"weight_init_fn\" not in kwargs:\n",
        "            self.apply(init_weights)\n",
        "        else:\n",
        "            self.apply(kwargs[\"weight_init_fn\"])\n",
        "\n",
        "\n",
        "    def forward(self, ids:torch.Tensor, length:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Feed the given token ids to the model.\n",
        "        :param ids: [batch size, seq len] batch of token ids.\n",
        "        :param length: [batch size] batch of length of the token ids.\n",
        "        :return: prediction of size [batch size, output dim].\n",
        "        \"\"\"\n",
        "        # Add your code here.\n",
        "        embeds = self.word_embedding(ids)\n",
        "        embed_padding = nn.utils.rnn.pack_padded_sequence(embeds, length, batch_first = True, enforce_sorted = False)\n",
        "        out, h = self.gru(embed_padding)\n",
        "        gru_out = h[-1]\n",
        "        \n",
        "        out = self.dropout(gru_out)\n",
        "        out = self.fc(out)\n",
        "        # out = self.sigmoid(out)\n",
        "        \n",
        "        # out = out.view(out.size(0), -1)\n",
        "        # out = out[:,-1]\n",
        "        \n",
        "        prediction = out\n",
        "        \n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP8LPLwHZ8HX"
      },
      "source": [
        "### Learning rate warmup. DO NOT TOUCH!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uWMM1fJ4Z8HX"
      },
      "outputs": [],
      "source": [
        "class ConstantWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer,\n",
        "        num_warmup_steps: int,\n",
        "    ):\n",
        "        self.num_warmup_steps = num_warmup_steps\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self._step_count <= self.num_warmup_steps:\n",
        "            # warmup\n",
        "            scale = 1.0 - (self.num_warmup_steps - self._step_count) / self.num_warmup_steps\n",
        "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
        "            self.last_lr = lr\n",
        "        else:\n",
        "            lr = self.base_lrs\n",
        "        return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Vgvw0PZ8HX"
      },
      "source": [
        "### Implement the training / validation iteration here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qXLkQSnS9xZw"
      },
      "outputs": [],
      "source": [
        "def train_and_test_model_with_hparams(hparams, model_type=\"lstm\", **kwargs):\n",
        "    # Seeding. DO NOT TOUCH! DO NOT TOUCH hparams.SEED!\n",
        "    # Set the random seeds.\n",
        "    CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "    torch.manual_seed(hparams.SEED)\n",
        "    random.seed(hparams.SEED)\n",
        "    np.random.seed(hparams.SEED)\n",
        "\n",
        "    x_train, x_valid, x_test, y_train, y_valid, y_test = load_imdb()\n",
        "    vocab = build_vocab(x_train, hparams=hparams)\n",
        "    vocab_size = len(vocab)\n",
        "    print(f'Length of vocabulary is {vocab_size}')\n",
        "\n",
        "    train_data = IMDB(x_train, y_train, vocab, hparams.MAX_LENGTH)\n",
        "    valid_data = IMDB(x_valid, y_valid, vocab, hparams.MAX_LENGTH)\n",
        "    test_data = IMDB(x_test, y_test, vocab, hparams.MAX_LENGTH)\n",
        "\n",
        "    collate = functools.partial(collate_fn, pad_index=hparams.PAD_INDEX)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate, shuffle=True)\n",
        "    valid_dataloader = torch.utils.data.DataLoader(\n",
        "        valid_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "    test_dataloader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "    \n",
        "    # Model\n",
        "    if \"override_models_with_gru\" in kwargs and kwargs[\"override_models_with_gru\"]:\n",
        "        model = GRU(\n",
        "            vocab_size, \n",
        "            hparams.EMBEDDING_DIM, \n",
        "            hparams.HIDDEN_DIM, \n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE, \n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    else:\n",
        "        model = LSTM(\n",
        "            vocab_size, \n",
        "            hparams.EMBEDDING_DIM, \n",
        "            hparams.HIDDEN_DIM, \n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE, \n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    num_params = count_parameters(model)\n",
        "    print(f'The model has {num_params:,} trainable parameters')\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimization. Lab 2 (a)(b) should choose one of them.\n",
        "    # DO NOT TOUCH optimizer-specific hyperparameters! (e.g., eps, momentum)\n",
        "    # DO NOT change optimizer implementations!\n",
        "    if hparams.OPTIM == \"sgd\":\n",
        "        optimizer = optim.SGD(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, momentum=.9)        \n",
        "    elif hparams.OPTIM == \"adagrad\":\n",
        "        optimizer = optim.Adagrad(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
        "    elif hparams.OPTIM == \"adam\":\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
        "    elif hparams.OPTIM == \"rmsprop\":\n",
        "        optimizer = optim.RMSprop(\n",
        "            model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6, momentum=.9)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Optimizer not implemented!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # Start training\n",
        "    best_valid_loss = float('inf')\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    valid_losses = []\n",
        "    valid_accs = [] \n",
        "\n",
        "    # Warmup Scheduler. DO NOT TOUCH!\n",
        "    WARMUP_STEPS = 200\n",
        "    lr_scheduler = ConstantWithWarmup(optimizer, WARMUP_STEPS)\n",
        "\n",
        "    for epoch in range(hparams.N_EPOCHS):\n",
        "        \n",
        "        # Your code: implement the training process and save the best model.\n",
        "        # epoch_losses, epoch_accs = train(train_dataloader, model, criterion, optimizer, lr_scheduler, device)\n",
        "        # temp_loss, temp_acc = train(train_dataloader, model, criterion, optimizer, lr_scheduler, device)\n",
        "        # train_losses.append(temp_loss)\n",
        "        # train_accs.append(temp_acc)\n",
        "        # print(train_losses, train_accs)\n",
        "        # temp_loss, temp_acc = evaluate(valid_dataloader, model, criterion, device)\n",
        "        # valid_losses.append(temp_loss)\n",
        "        # valid_accs.append(temp_acc)\n",
        "        train_losses, train_accs = train(train_dataloader, model, criterion, optimizer, lr_scheduler, device)\n",
        "        valid_losses, valid_accs = evaluate(valid_dataloader, model, criterion, device)\n",
        "        \n",
        "        epoch_train_loss = np.mean(train_losses)\n",
        "        epoch_train_acc = np.mean(train_accs)\n",
        "        epoch_valid_loss = np.mean(valid_losses)\n",
        "        epoch_valid_acc = np.mean(valid_accs)\n",
        "\n",
        "        # Save the model that achieves the smallest validation loss.\n",
        "        if epoch_valid_loss < best_valid_loss:\n",
        "            # Your code: save the best model somewhere (no need to submit it to Sakai)\n",
        "          best_valid_loss = epoch_valid_loss\n",
        "          if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "            print(\"making folder\")\n",
        "            os.makedirs(CHECKPOINT_FOLDER)\n",
        "          print(\"Saving ...\")\n",
        "          state = {'state_dict': model.state_dict(),\n",
        "                  'epoch': epoch,\n",
        "                  'lr': hparams.LR}\n",
        "          torch.save(state, os.path.join(CHECKPOINT_FOLDER, f'{model_type}.pth'))\n",
        "            # pass\n",
        "\n",
        "        print(f'epoch: {epoch+1}')\n",
        "        print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
        "        print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n",
        "\n",
        "\n",
        "    # Your Code: Load the best model's weights.\n",
        "    load_model = os.path.join(CHECKPOINT_FOLDER, f'{model_type}.pth')\n",
        "    state_dict = torch.load(load_model) # change the path to your own checkpoint file\n",
        "    model.load_state_dict(state_dict['state_dict'])\n",
        "    model.cuda()\n",
        "    # model = \n",
        "\n",
        "    # Your Code: evaluate test loss on testing dataset (NOT Validation)\n",
        "\n",
        "    test_loss, test_acc = [], []\n",
        "    total, correct = 0, 0\n",
        "    # with torch.no_grad():\n",
        "    #   for batch_idx, (input, target) in enumerate(test_dataloader):\n",
        "    #     input = input.type(torch.cuda.FloatTensor)\n",
        "    #     output = model(input)\n",
        "    #     output = output.to(device)\n",
        "\n",
        "    #     loss = criterion(output, target.to(device))\n",
        "    #     test_losses += loss\n",
        "        \n",
        "    #     prediction = torch.argmax(output.data, axis = 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(test_dataloader, desc='testing...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids, length)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            test_loss.append(loss.item())\n",
        "            test_acc.append(accuracy.item())\n",
        "\n",
        "\n",
        "    # get_accuracy(prediction, label):\n",
        "\n",
        "    # test_loss, test_acc = test_losses, get_accuracy(prediction, target)\n",
        "\n",
        "    epoch_test_loss = np.mean(test_loss)\n",
        "    epoch_test_acc = np.mean(test_acc)\n",
        "    print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')\n",
        "    \n",
        "    # Free memory for later usage.\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    return {\n",
        "        'num_params': num_params,\n",
        "        \"test_loss\": epoch_test_loss,\n",
        "        \"test_acc\": epoch_test_acc,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKCu4rPBA2Sp"
      },
      "source": [
        "### Lab 1 (f): Train model with original hyperparameters, for LSTM.\n",
        "\n",
        "Train the model with default hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUL1bQ9dZ8HY",
        "outputId": "e23fabd9-add6-4bf8-bddb-e87e2919ee53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 102,196 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 43.10it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 88.88it/s]\n",
            "making folder\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.497\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.94it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 89.54it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.693, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.520\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.50it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.90it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.694, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.503\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.80it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 91.25it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.694, train_acc: 0.499\n",
            "valid_loss: 0.694, valid_acc: 0.497\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.51it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 90.80it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.694, train_acc: 0.500\n",
            "valid_loss: 0.693, valid_acc: 0.497\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 86.33it/s]\n",
            "test_loss: 0.693, test_acc: 0.499\n"
          ]
        }
      ],
      "source": [
        "org_hyperparams = HyperParams()\n",
        "_ = train_and_test_model_with_hparams(org_hyperparams, \"lstm_1layer_base_sgd_e32_h100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwQtXY1IZ8HY"
      },
      "source": [
        "### Lab 1 (h) Train GRU with vanilla hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "drpcc5mDZ8HY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2342363-3fb0-4586-8f46-d2564d0b43b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 91,896 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.76it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.95it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.500\n",
            "valid_loss: 0.693, valid_acc: 0.503\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 52.07it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 88.93it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.694, train_acc: 0.499\n",
            "valid_loss: 0.693, valid_acc: 0.503\n",
            "training...: 100%|██████████| 365/365 [00:06<00:00, 52.94it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 90.97it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.694, train_acc: 0.503\n",
            "valid_loss: 0.693, valid_acc: 0.502\n",
            "training...: 100%|██████████| 365/365 [00:06<00:00, 52.92it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.29it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.694, train_acc: 0.500\n",
            "valid_loss: 0.693, valid_acc: 0.497\n",
            "training...: 100%|██████████| 365/365 [00:06<00:00, 52.85it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 89.79it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.694, train_acc: 0.502\n",
            "valid_loss: 0.693, valid_acc: 0.497\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 86.13it/s]\n",
            "test_loss: 0.693, test_acc: 0.506\n"
          ]
        }
      ],
      "source": [
        "org_hyperparams = HyperParams()\n",
        "_ = train_and_test_model_with_hparams(org_hyperparams, \"gru_1layer_base_sgd_e32_h100\", override_models_with_gru=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B4-GTE-Z8HY"
      },
      "source": [
        "### Lab 2 (a) Study of LSTM Optimizers. Hint: For adaptive optimizers, we recommend using a learning rate of 0.001 (instead of 0.01)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "V4z3tn7jZ8HY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25118e1b-b693-4eef-b3df-bf45931378d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with optimizer: sgd\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 102,196 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 47.97it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 81.45it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.499\n",
            "valid_loss: 0.693, valid_acc: 0.497\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.63it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 86.15it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.693, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.503\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.97it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.98it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.693, train_acc: 0.500\n",
            "valid_loss: 0.693, valid_acc: 0.497\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.10it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.10it/s]\n",
            "Saving ...\n",
            "epoch: 4\n",
            "train_loss: 0.693, train_acc: 0.497\n",
            "valid_loss: 0.693, valid_acc: 0.503\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.36it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 86.88it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.693, train_acc: 0.498\n",
            "valid_loss: 0.693, valid_acc: 0.497\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 82.60it/s]\n",
            "test_loss: 0.693, test_acc: 0.499\n",
            "training with optimizer: adagrad\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 102,196 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.84it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.14it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.505\n",
            "valid_loss: 0.693, valid_acc: 0.601\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.17it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 81.92it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.692, train_acc: 0.556\n",
            "valid_loss: 0.677, valid_acc: 0.665\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.01it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 81.38it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.602, train_acc: 0.753\n",
            "valid_loss: 0.603, valid_acc: 0.735\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 43.73it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.37it/s]\n",
            "Saving ...\n",
            "epoch: 4\n",
            "train_loss: 0.526, train_acc: 0.817\n",
            "valid_loss: 0.553, valid_acc: 0.790\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.67it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.13it/s]\n",
            "Saving ...\n",
            "epoch: 5\n",
            "train_loss: 0.496, train_acc: 0.833\n",
            "valid_loss: 0.522, valid_acc: 0.813\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 82.45it/s]\n",
            "test_loss: 0.519, test_acc: 0.815\n",
            "training with optimizer: adam\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 102,196 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 47.61it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 86.55it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.697, train_acc: 0.558\n",
            "valid_loss: 0.654, valid_acc: 0.742\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.68it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.61it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.492, train_acc: 0.772\n",
            "valid_loss: 0.305, valid_acc: 0.873\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.59it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.25it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.215, train_acc: 0.919\n",
            "valid_loss: 0.287, valid_acc: 0.885\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.45it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 84.71it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.119, train_acc: 0.962\n",
            "valid_loss: 0.338, valid_acc: 0.869\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 47.97it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.03it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.077, train_acc: 0.978\n",
            "valid_loss: 0.336, valid_acc: 0.887\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 81.26it/s]\n",
            "test_loss: 0.306, test_acc: 0.873\n",
            "training with optimizer: rmsprop\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 102,196 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.15it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.58it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.647, train_acc: 0.626\n",
            "valid_loss: 0.428, valid_acc: 0.819\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.29it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.58it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.353, train_acc: 0.855\n",
            "valid_loss: 0.371, valid_acc: 0.859\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.28it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.87it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.202, train_acc: 0.926\n",
            "valid_loss: 0.341, valid_acc: 0.879\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.86it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 84.30it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.118, train_acc: 0.961\n",
            "valid_loss: 0.392, valid_acc: 0.876\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.08it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.03it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.063, train_acc: 0.980\n",
            "valid_loss: 0.533, valid_acc: 0.872\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 82.52it/s]\n",
            "test_loss: 0.361, test_acc: 0.867\n",
            "[0.498710332598005, 0.8153770032383146, 0.8733135109856015, 0.8666666871025449]\n"
          ]
        }
      ],
      "source": [
        "h = HyperParams()\n",
        "h.LR = 0.001\n",
        "h_list = [\"sgd\", \"adagrad\", \"adam\", \"rmsprop\"]\n",
        "test_acc = []\n",
        "for i in h_list:\n",
        "  print(\"training with optimizer: \" + i)\n",
        "  print(\"\\n---------------------------------------\\n\")\n",
        "  h.OPTIM = i\n",
        "  name = \"lstm_1layer_base_\" + i + \"_e32_h100\"\n",
        "  _ = train_and_test_model_with_hparams(h, name)\n",
        "  test_acc.append(_.get(\"test_acc\"))\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8ZNzK7_Z8HY"
      },
      "source": [
        "### Lab 2 (b): Study of GRU Optimizers. Hint: For adaptive optimizers, we recommend using a learning rate of 0.001 (instead of 0.01)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "H2eKmTk-Z8HY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4cc0b15-773f-4738-ce36-0fc64439a5e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with optimizer: sgd\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 91,896 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.37it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 84.30it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.501\n",
            "valid_loss: 0.693, valid_acc: 0.503\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.81it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.73it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.693, train_acc: 0.499\n",
            "valid_loss: 0.693, valid_acc: 0.503\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.38it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 84.18it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.693, train_acc: 0.499\n",
            "valid_loss: 0.693, valid_acc: 0.503\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.91it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.07it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.693, train_acc: 0.499\n",
            "valid_loss: 0.693, valid_acc: 0.497\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.08it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 88.96it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.693, train_acc: 0.500\n",
            "valid_loss: 0.693, valid_acc: 0.497\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 82.50it/s]\n",
            "test_loss: 0.693, test_acc: 0.499\n",
            "training with optimizer: adagrad\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 91,896 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.84it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.61it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.505\n",
            "valid_loss: 0.693, valid_acc: 0.555\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.27it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.02it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.682, train_acc: 0.587\n",
            "valid_loss: 0.595, valid_acc: 0.794\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.87it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.75it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.531, train_acc: 0.820\n",
            "valid_loss: 0.510, valid_acc: 0.824\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.13it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 88.05it/s]\n",
            "Saving ...\n",
            "epoch: 4\n",
            "train_loss: 0.450, train_acc: 0.862\n",
            "valid_loss: 0.458, valid_acc: 0.850\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.04it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.39it/s]\n",
            "Saving ...\n",
            "epoch: 5\n",
            "train_loss: 0.399, train_acc: 0.880\n",
            "valid_loss: 0.428, valid_acc: 0.861\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 83.72it/s]\n",
            "test_loss: 0.426, test_acc: 0.858\n",
            "training with optimizer: adam\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 91,896 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.23it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 81.89it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.607, train_acc: 0.639\n",
            "valid_loss: 0.332, valid_acc: 0.856\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.15it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.26it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.243, train_acc: 0.905\n",
            "valid_loss: 0.283, valid_acc: 0.887\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.79it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 86.92it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.108, train_acc: 0.964\n",
            "valid_loss: 0.348, valid_acc: 0.874\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.52it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 83.84it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.048, train_acc: 0.986\n",
            "valid_loss: 0.514, valid_acc: 0.880\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.98it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.95it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.022, train_acc: 0.994\n",
            "valid_loss: 0.529, valid_acc: 0.880\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 85.88it/s]\n",
            "test_loss: 0.287, test_acc: 0.883\n",
            "training with optimizer: rmsprop\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 91,896 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.39it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 84.34it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.469, train_acc: 0.750\n",
            "valid_loss: 0.317, valid_acc: 0.870\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 51.43it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.29it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.182, train_acc: 0.934\n",
            "valid_loss: 0.294, valid_acc: 0.885\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.99it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 86.00it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.082, train_acc: 0.972\n",
            "valid_loss: 0.360, valid_acc: 0.870\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.23it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 84.70it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.042, train_acc: 0.986\n",
            "valid_loss: 0.512, valid_acc: 0.867\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.75it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 83.33it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.025, train_acc: 0.991\n",
            "valid_loss: 0.525, valid_acc: 0.875\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 82.40it/s]\n",
            "test_loss: 0.301, test_acc: 0.880\n",
            "[0.498710332598005, 0.8577381162416368, 0.8834325597399757, 0.8804563675607954]\n"
          ]
        }
      ],
      "source": [
        "h = HyperParams()\n",
        "h.LR = 0.001\n",
        "h_list = [\"sgd\", \"adagrad\", \"adam\", \"rmsprop\"]\n",
        "test_acc = []\n",
        "for i in h_list:\n",
        "  print(\"training with optimizer: \" + i)\n",
        "  print(\"\\n---------------------------------------\\n\")\n",
        "  h.OPTIM = i\n",
        "  name = \"gru_1layer_base_\" + i + \"_e32_h100\"\n",
        "  _ = train_and_test_model_with_hparams(h, name, override_models_with_gru=True)\n",
        "  test_acc.append(_.get(\"test_acc\"))\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89GlxXKZ8HY"
      },
      "source": [
        "### Lab 2 (c) Deeper LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0McapbtlZ8HZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1c7d2d-814b-4097-e9a6-a2068ff8d1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with number of layers: 1\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 102,196 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.72it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.10it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.697, train_acc: 0.558\n",
            "valid_loss: 0.654, valid_acc: 0.742\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.15it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.96it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.492, train_acc: 0.772\n",
            "valid_loss: 0.305, valid_acc: 0.873\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.41it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 88.93it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.215, train_acc: 0.919\n",
            "valid_loss: 0.287, valid_acc: 0.885\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.61it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.40it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.119, train_acc: 0.962\n",
            "valid_loss: 0.338, valid_acc: 0.869\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 49.52it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 90.10it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.077, train_acc: 0.978\n",
            "valid_loss: 0.336, valid_acc: 0.887\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 86.90it/s]\n",
            "test_loss: 0.306, test_acc: 0.873\n",
            "training with number of layers: 2\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 182,996 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 29.82it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 71.13it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.678, train_acc: 0.564\n",
            "valid_loss: 0.541, valid_acc: 0.789\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 34.30it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 69.53it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.317, train_acc: 0.869\n",
            "valid_loss: 0.288, valid_acc: 0.891\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 35.01it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 72.16it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.149, train_acc: 0.947\n",
            "valid_loss: 0.299, valid_acc: 0.888\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 34.97it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 72.26it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.076, train_acc: 0.976\n",
            "valid_loss: 0.388, valid_acc: 0.872\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 34.78it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 71.53it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.037, train_acc: 0.989\n",
            "valid_loss: 0.428, valid_acc: 0.858\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 70.80it/s]\n",
            "test_loss: 0.312, test_acc: 0.882\n",
            "training with number of layers: 3\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 263,796 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 27.36it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.93it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.522\n",
            "valid_loss: 0.684, valid_acc: 0.633\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 27.18it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.16it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.390, train_acc: 0.830\n",
            "valid_loss: 0.290, valid_acc: 0.883\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 27.54it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.86it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.181, train_acc: 0.936\n",
            "valid_loss: 0.286, valid_acc: 0.885\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 27.45it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.97it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.098, train_acc: 0.969\n",
            "valid_loss: 0.409, valid_acc: 0.854\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 27.23it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 58.45it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.046, train_acc: 0.988\n",
            "valid_loss: 0.466, valid_acc: 0.881\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 59.24it/s]\n",
            "test_loss: 0.303, test_acc: 0.877\n",
            "training with number of layers: 4\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 344,596 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 22.57it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 38.55it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.693, train_acc: 0.503\n",
            "valid_loss: 0.692, valid_acc: 0.513\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 21.95it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 52.30it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.529, train_acc: 0.719\n",
            "valid_loss: 0.373, valid_acc: 0.843\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 22.66it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 52.84it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.287, train_acc: 0.887\n",
            "valid_loss: 0.289, valid_acc: 0.885\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 22.46it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 51.94it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.175, train_acc: 0.939\n",
            "valid_loss: 0.355, valid_acc: 0.870\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 22.53it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 52.37it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.095, train_acc: 0.971\n",
            "valid_loss: 0.378, valid_acc: 0.875\n",
            "testing...: 100%|██████████| 105/105 [00:02<00:00, 52.43it/s]\n",
            "test_loss: 0.300, test_acc: 0.881\n",
            "[0.8733135109856015, 0.8815476372128441, 0.8768849389893668, 0.8810516045207069]\n"
          ]
        }
      ],
      "source": [
        "# N_LAYERS\n",
        "h = HyperParams()\n",
        "h.LR = 0.001\n",
        "h.OPTIM = \"adam\"\n",
        "h_list = [1, 2, 3, 4]\n",
        "test_acc = []\n",
        "for i in h_list:\n",
        "  print(\"training with number of layers: \" + str(i))\n",
        "  print(\"\\n---------------------------------------\\n\")\n",
        "  h.N_LAYERS = i\n",
        "  name = \"lstm_1layer_base_\" + str(i) + \"_e32_h100\"\n",
        "  _ = train_and_test_model_with_hparams(h, name)\n",
        "  test_acc.append(_.get(\"test_acc\"))\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYOUD9i4Z8HZ"
      },
      "source": [
        "### Lab 2 (d) Wider LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xA5SqxaZZ8HZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d43498-5442-4ea1-adbb-42b3b63e63ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with hidden_dimension: 100\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 182,996 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 33.73it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 71.64it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.678, train_acc: 0.564\n",
            "valid_loss: 0.541, valid_acc: 0.789\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 34.89it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 71.53it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.317, train_acc: 0.869\n",
            "valid_loss: 0.288, valid_acc: 0.891\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 35.20it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 72.31it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.149, train_acc: 0.947\n",
            "valid_loss: 0.299, valid_acc: 0.888\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 34.42it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 70.12it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.076, train_acc: 0.976\n",
            "valid_loss: 0.388, valid_acc: 0.872\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 35.01it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 72.90it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.037, train_acc: 0.989\n",
            "valid_loss: 0.428, valid_acc: 0.858\n",
            "testing...: 100%|██████████| 105/105 [00:02<00:00, 50.87it/s]\n",
            "test_loss: 0.312, test_acc: 0.882\n",
            "training with hidden_dimension: 150\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 334,096 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 29.97it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 68.41it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.625, train_acc: 0.621\n",
            "valid_loss: 0.497, valid_acc: 0.763\n",
            "training...: 100%|██████████| 365/365 [00:11<00:00, 32.76it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 68.09it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.296, train_acc: 0.881\n",
            "valid_loss: 0.319, valid_acc: 0.866\n",
            "training...: 100%|██████████| 365/365 [00:10<00:00, 33.36it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 68.48it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.151, train_acc: 0.946\n",
            "valid_loss: 0.310, valid_acc: 0.881\n",
            "training...: 100%|██████████| 365/365 [00:11<00:00, 32.95it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 68.42it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.080, train_acc: 0.974\n",
            "valid_loss: 0.437, valid_acc: 0.874\n",
            "training...: 100%|██████████| 365/365 [00:11<00:00, 33.14it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 69.47it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.037, train_acc: 0.989\n",
            "valid_loss: 0.542, valid_acc: 0.859\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 68.00it/s]\n",
            "test_loss: 0.328, test_acc: 0.877\n",
            "training with hidden_dimension: 200\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 545,196 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.99it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 62.19it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.630, train_acc: 0.619\n",
            "valid_loss: 0.640, valid_acc: 0.719\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 29.26it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 62.07it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.270, train_acc: 0.893\n",
            "valid_loss: 0.284, valid_acc: 0.888\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 29.23it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 61.91it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.139, train_acc: 0.952\n",
            "valid_loss: 0.316, valid_acc: 0.891\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 29.09it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 62.70it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.070, train_acc: 0.978\n",
            "valid_loss: 0.404, valid_acc: 0.883\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 29.16it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 61.68it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.041, train_acc: 0.988\n",
            "valid_loss: 0.440, valid_acc: 0.860\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 60.95it/s]\n",
            "test_loss: 0.308, test_acc: 0.882\n",
            "training with hidden_dimension: 250\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 816,296 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:15<00:00, 23.80it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 53.01it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.662, train_acc: 0.586\n",
            "valid_loss: 0.467, valid_acc: 0.787\n",
            "training...: 100%|██████████| 365/365 [00:14<00:00, 24.50it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 53.01it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.331, train_acc: 0.861\n",
            "valid_loss: 0.342, valid_acc: 0.856\n",
            "training...: 100%|██████████| 365/365 [00:14<00:00, 24.66it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 52.46it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.162, train_acc: 0.940\n",
            "valid_loss: 0.366, valid_acc: 0.846\n",
            "training...: 100%|██████████| 365/365 [00:14<00:00, 24.47it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 51.41it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.080, train_acc: 0.975\n",
            "valid_loss: 0.369, valid_acc: 0.877\n",
            "training...: 100%|██████████| 365/365 [00:14<00:00, 24.53it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 51.88it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.044, train_acc: 0.987\n",
            "valid_loss: 0.507, valid_acc: 0.872\n",
            "testing...: 100%|██████████| 105/105 [00:02<00:00, 51.55it/s]\n",
            "test_loss: 0.349, test_acc: 0.851\n",
            "training with hidden_dimension: 300\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 1,147,396 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:20<00:00, 17.48it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 44.72it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.692, train_acc: 0.536\n",
            "valid_loss: 0.661, valid_acc: 0.706\n",
            "training...: 100%|██████████| 365/365 [00:19<00:00, 18.26it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 43.85it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.429, train_acc: 0.817\n",
            "valid_loss: 0.344, valid_acc: 0.851\n",
            "training...: 100%|██████████| 365/365 [00:19<00:00, 18.61it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 43.31it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.249, train_acc: 0.902\n",
            "valid_loss: 0.304, valid_acc: 0.882\n",
            "training...: 100%|██████████| 365/365 [00:23<00:00, 15.68it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 44.30it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.130, train_acc: 0.955\n",
            "valid_loss: 0.339, valid_acc: 0.872\n",
            "training...: 100%|██████████| 365/365 [00:19<00:00, 18.38it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 36.60it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.069, train_acc: 0.979\n",
            "valid_loss: 0.377, valid_acc: 0.865\n",
            "testing...: 100%|██████████| 105/105 [00:02<00:00, 38.79it/s]\n",
            "test_loss: 0.318, test_acc: 0.876\n",
            "[0.8815476372128441, 0.876587320509411, 0.8817460497220357, 0.8514881128356571, 0.8758928764434087]\n"
          ]
        }
      ],
      "source": [
        "# Hidden_dim\n",
        "h = HyperParams()\n",
        "h.LR = 0.001\n",
        "h.N_LAYERS = 2\n",
        "h.OPTIM = \"adam\"\n",
        "h_list = [100, 150, 200, 250, 300]\n",
        "test_acc = []\n",
        "for i in h_list:\n",
        "  print(\"training with hidden_dimension: \" + str(i))\n",
        "  print(\"\\n---------------------------------------\\n\")\n",
        "  h.HIDDEN_DIM = i\n",
        "  name = \"lstm_1layer_base_\" + str(i) + \"_e32_h100\"\n",
        "  _ = train_and_test_model_with_hparams(h, name)\n",
        "  test_acc.append(_.get(\"test_acc\"))\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6xPtWe6Z8HZ"
      },
      "source": [
        "### Lab 2 (e) Larger Embedding Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "k24jyflfZ8HZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6649b9c1-e583-4df7-9def-c300cc0ac176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with embedding_dim: 1\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 545,196 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.45it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 61.96it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.630, train_acc: 0.619\n",
            "valid_loss: 0.640, valid_acc: 0.719\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.43it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.94it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.270, train_acc: 0.893\n",
            "valid_loss: 0.284, valid_acc: 0.888\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.53it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 62.51it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.139, train_acc: 0.952\n",
            "valid_loss: 0.316, valid_acc: 0.891\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.63it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 63.26it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.070, train_acc: 0.978\n",
            "valid_loss: 0.404, valid_acc: 0.883\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.80it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 61.88it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.041, train_acc: 0.988\n",
            "valid_loss: 0.440, valid_acc: 0.860\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 59.62it/s]\n",
            "test_loss: 0.308, test_acc: 0.882\n",
            "training with embedding_dim: 16\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 1,469,106 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 26.90it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 61.15it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.568, train_acc: 0.681\n",
            "valid_loss: 0.321, valid_acc: 0.871\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.40it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 61.50it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.230, train_acc: 0.912\n",
            "valid_loss: 0.367, valid_acc: 0.877\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.10it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.64it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.107, train_acc: 0.965\n",
            "valid_loss: 0.369, valid_acc: 0.874\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.23it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.77it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.065, train_acc: 0.979\n",
            "valid_loss: 0.484, valid_acc: 0.867\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 28.27it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 61.45it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.065, train_acc: 0.977\n",
            "valid_loss: 0.595, valid_acc: 0.828\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 61.64it/s]\n",
            "test_loss: 0.321, test_acc: 0.870\n",
            "training with embedding_dim: 64\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 4,425,618 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 26.83it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.36it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.513, train_acc: 0.717\n",
            "valid_loss: 0.460, valid_acc: 0.783\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 26.75it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 58.68it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.222, train_acc: 0.918\n",
            "valid_loss: 0.322, valid_acc: 0.865\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 26.54it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 58.74it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.102, train_acc: 0.966\n",
            "valid_loss: 0.393, valid_acc: 0.869\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 26.64it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 61.08it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.048, train_acc: 0.986\n",
            "valid_loss: 0.497, valid_acc: 0.862\n",
            "training...: 100%|██████████| 365/365 [00:13<00:00, 26.78it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 59.70it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.032, train_acc: 0.990\n",
            "valid_loss: 0.645, valid_acc: 0.863\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 60.85it/s]\n",
            "test_loss: 0.334, test_acc: 0.863\n",
            "training with embedding_dim: 128\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 8,367,634 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:14<00:00, 24.67it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 59.50it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.495, train_acc: 0.737\n",
            "valid_loss: 0.345, valid_acc: 0.855\n",
            "training...: 100%|██████████| 365/365 [00:14<00:00, 24.98it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.71it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.227, train_acc: 0.913\n",
            "valid_loss: 0.377, valid_acc: 0.851\n",
            "training...: 100%|██████████| 365/365 [00:14<00:00, 24.81it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 58.72it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.114, train_acc: 0.961\n",
            "valid_loss: 0.409, valid_acc: 0.863\n",
            "training...: 100%|██████████| 365/365 [00:15<00:00, 23.90it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 57.63it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.061, train_acc: 0.981\n",
            "valid_loss: 0.447, valid_acc: 0.869\n",
            "training...: 100%|██████████| 365/365 [00:14<00:00, 24.68it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 57.64it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.035, train_acc: 0.989\n",
            "valid_loss: 0.586, valid_acc: 0.860\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 58.02it/s]\n",
            "test_loss: 0.361, test_acc: 0.850\n",
            "training with embedding_dim: 256\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 16,251,666 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 21.81it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 56.52it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.466, train_acc: 0.762\n",
            "valid_loss: 0.395, valid_acc: 0.838\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 21.82it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 56.23it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.227, train_acc: 0.918\n",
            "valid_loss: 0.358, valid_acc: 0.850\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 21.85it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 57.43it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.097, train_acc: 0.967\n",
            "valid_loss: 0.458, valid_acc: 0.876\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 22.05it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 55.98it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.052, train_acc: 0.983\n",
            "valid_loss: 0.477, valid_acc: 0.850\n",
            "training...: 100%|██████████| 365/365 [00:16<00:00, 22.02it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 56.58it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.033, train_acc: 0.989\n",
            "valid_loss: 0.617, valid_acc: 0.860\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 55.46it/s]\n",
            "test_loss: 0.381, test_acc: 0.849\n",
            "[0.8817460497220357, 0.870337320509411, 0.862797641186487, 0.8497024025235858, 0.8489087490808396]\n"
          ]
        }
      ],
      "source": [
        "# EMBEDDING_DIM\n",
        "h = HyperParams()\n",
        "h.LR = 0.001\n",
        "h.N_LAYERS = 2\n",
        "h.OPTIM = \"adam\"\n",
        "h.HIDDEN_DIM = 200\n",
        "h_list = [1, 16, 64, 128, 256]\n",
        "test_acc = []\n",
        "for i in h_list:\n",
        "  print(\"training with embedding_dim: \" + str(i))\n",
        "  print(\"\\n---------------------------------------\\n\")\n",
        "  h.EMBEDDING_DIM = i\n",
        "  name = \"lstm_1layer_base_\" + str(i) + \"_e32_h100\"\n",
        "  _ = train_and_test_model_with_hparams(h, name)\n",
        "  test_acc.append(_.get(\"test_acc\"))\n",
        "print(test_acc) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHFyRRp_Z8Ha"
      },
      "source": [
        "### Lab 2(f) Compound scaling of embedding_dim, hidden_dim, layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_C4nd_PhZ8Ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752fbcba-2c34-4e11-b999-c14db94de3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------\n",
            "training with hidden_dim: 100, embed_dim: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 102,196 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.32it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 82.84it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.697, train_acc: 0.558\n",
            "valid_loss: 0.654, valid_acc: 0.742\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 44.38it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 84.43it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.492, train_acc: 0.772\n",
            "valid_loss: 0.305, valid_acc: 0.873\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.77it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.59it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.215, train_acc: 0.919\n",
            "valid_loss: 0.287, valid_acc: 0.885\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 41.00it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.12it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.119, train_acc: 0.962\n",
            "valid_loss: 0.338, valid_acc: 0.869\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 47.44it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 83.76it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.077, train_acc: 0.978\n",
            "valid_loss: 0.336, valid_acc: 0.887\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 83.49it/s]\n",
            "test_loss: 0.306, test_acc: 0.873\n",
            "\n",
            "---------------------------------------\n",
            "training with hidden_dim: 100, embed_dim: 16\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 1,020,106 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.11it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 86.93it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.521, train_acc: 0.711\n",
            "valid_loss: 0.324, valid_acc: 0.876\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 48.08it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 70.14it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.230, train_acc: 0.916\n",
            "valid_loss: 0.293, valid_acc: 0.880\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.08it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 89.18it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.105, train_acc: 0.966\n",
            "valid_loss: 0.353, valid_acc: 0.871\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.22it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.00it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.051, train_acc: 0.985\n",
            "valid_loss: 0.560, valid_acc: 0.878\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 50.49it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 82.22it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.040, train_acc: 0.988\n",
            "valid_loss: 0.540, valid_acc: 0.841\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 84.37it/s]\n",
            "test_loss: 0.309, test_acc: 0.876\n",
            "\n",
            "---------------------------------------\n",
            "training with hidden_dim: 100, embed_dim: 64\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 3,957,418 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.00it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 86.43it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.464, train_acc: 0.749\n",
            "valid_loss: 0.328, valid_acc: 0.868\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.93it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.85it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.189, train_acc: 0.932\n",
            "valid_loss: 0.320, valid_acc: 0.887\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.79it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 83.34it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.080, train_acc: 0.974\n",
            "valid_loss: 0.379, valid_acc: 0.877\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.51it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 87.81it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.045, train_acc: 0.986\n",
            "valid_loss: 0.790, valid_acc: 0.859\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 47.50it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 88.25it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.047, train_acc: 0.984\n",
            "valid_loss: 0.614, valid_acc: 0.873\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 87.57it/s]\n",
            "test_loss: 0.337, test_acc: 0.876\n",
            "\n",
            "---------------------------------------\n",
            "training with hidden_dim: 150, embed_dim: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 152,896 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 47.10it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 83.53it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.684, train_acc: 0.548\n",
            "valid_loss: 0.624, valid_acc: 0.707\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.34it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 83.72it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.385, train_acc: 0.838\n",
            "valid_loss: 0.288, valid_acc: 0.882\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 47.58it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 82.45it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.180, train_acc: 0.935\n",
            "valid_loss: 0.280, valid_acc: 0.885\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 47.23it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 80.88it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.100, train_acc: 0.968\n",
            "valid_loss: 0.339, valid_acc: 0.889\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.19it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 84.39it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.059, train_acc: 0.983\n",
            "valid_loss: 0.481, valid_acc: 0.882\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 78.88it/s]\n",
            "test_loss: 0.294, test_acc: 0.882\n",
            "\n",
            "---------------------------------------\n",
            "training with hidden_dim: 150, embed_dim: 16\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 1,073,806 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 45.84it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 82.84it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.528, train_acc: 0.700\n",
            "valid_loss: 0.316, valid_acc: 0.876\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.22it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 80.74it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.238, train_acc: 0.914\n",
            "valid_loss: 0.316, valid_acc: 0.880\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.31it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 80.23it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.107, train_acc: 0.965\n",
            "valid_loss: 0.347, valid_acc: 0.878\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.35it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 80.59it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.056, train_acc: 0.983\n",
            "valid_loss: 0.460, valid_acc: 0.867\n",
            "training...: 100%|██████████| 365/365 [00:07<00:00, 46.85it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 85.13it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.037, train_acc: 0.988\n",
            "valid_loss: 0.520, valid_acc: 0.862\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 82.01it/s]\n",
            "test_loss: 0.328, test_acc: 0.869\n",
            "\n",
            "---------------------------------------\n",
            "training with hidden_dim: 150, embed_dim: 64\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 4,020,718 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 43.35it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 81.04it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.539, train_acc: 0.710\n",
            "valid_loss: 0.394, valid_acc: 0.825\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 41.07it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 83.17it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.235, train_acc: 0.910\n",
            "valid_loss: 0.319, valid_acc: 0.872\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 44.57it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 80.81it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.102, train_acc: 0.967\n",
            "valid_loss: 0.376, valid_acc: 0.859\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 44.16it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 80.95it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.055, train_acc: 0.983\n",
            "valid_loss: 0.562, valid_acc: 0.863\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 44.13it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 78.88it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.033, train_acc: 0.990\n",
            "valid_loss: 0.625, valid_acc: 0.860\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 79.16it/s]\n",
            "test_loss: 0.333, test_acc: 0.867\n",
            "\n",
            "---------------------------------------\n",
            "training with hidden_dim: 200, embed_dim: 1\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 223,596 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 43.18it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 76.20it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.663, train_acc: 0.576\n",
            "valid_loss: 3.813, valid_acc: 0.716\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 44.33it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 75.66it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.343, train_acc: 0.854\n",
            "valid_loss: 0.290, valid_acc: 0.889\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 43.97it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 76.10it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.160, train_acc: 0.945\n",
            "valid_loss: 0.287, valid_acc: 0.892\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 44.29it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 75.17it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.085, train_acc: 0.974\n",
            "valid_loss: 0.361, valid_acc: 0.891\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 44.25it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 76.11it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.041, train_acc: 0.989\n",
            "valid_loss: 0.403, valid_acc: 0.881\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 74.36it/s]\n",
            "test_loss: 0.310, test_acc: 0.882\n",
            "\n",
            "---------------------------------------\n",
            "training with hidden_dim: 200, embed_dim: 16\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 1,147,506 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 42.80it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 75.97it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.550, train_acc: 0.677\n",
            "valid_loss: 0.318, valid_acc: 0.871\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 44.07it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 72.06it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.226, train_acc: 0.914\n",
            "valid_loss: 0.319, valid_acc: 0.866\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 43.60it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 74.64it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.098, train_acc: 0.968\n",
            "valid_loss: 0.365, valid_acc: 0.873\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 43.41it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 76.12it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.059, train_acc: 0.981\n",
            "valid_loss: 0.518, valid_acc: 0.869\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 43.84it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 76.11it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.034, train_acc: 0.990\n",
            "valid_loss: 0.676, valid_acc: 0.867\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 75.47it/s]\n",
            "test_loss: 0.329, test_acc: 0.862\n",
            "\n",
            "---------------------------------------\n",
            "training with hidden_dim: 200, embed_dim: 64\n",
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 4,104,018 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:09<00:00, 40.02it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 75.08it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.478, train_acc: 0.744\n",
            "valid_loss: 0.346, valid_acc: 0.867\n",
            "training...: 100%|██████████| 365/365 [00:09<00:00, 40.16it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 74.44it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.218, train_acc: 0.921\n",
            "valid_loss: 0.342, valid_acc: 0.876\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 40.94it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 75.41it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.112, train_acc: 0.963\n",
            "valid_loss: 0.393, valid_acc: 0.864\n",
            "training...: 100%|██████████| 365/365 [00:09<00:00, 40.20it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 54.87it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.057, train_acc: 0.982\n",
            "valid_loss: 0.525, valid_acc: 0.869\n",
            "training...: 100%|██████████| 365/365 [00:08<00:00, 40.68it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 76.50it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.039, train_acc: 0.988\n",
            "valid_loss: 0.642, valid_acc: 0.854\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 75.62it/s]\n",
            "test_loss: 0.357, test_acc: 0.869\n",
            "0.8815476406188238\n",
            "{'layer': 4, 'hidden': 150, 'embed': 1}\n"
          ]
        }
      ],
      "source": [
        "from nltk.metrics.distance import jaro_similarity\n",
        "h = HyperParams()\n",
        "h.LR = 0.001\n",
        "h.OPTIM = \"adam\"\n",
        "h.N_LAYERS = 1\n",
        "hidden_dim_list = [100, 150, 200]\n",
        "embed_dim_list = [1, 16, 64]\n",
        "test_acc = []\n",
        "res = 0\n",
        "for k in hidden_dim_list:\n",
        "  for j in embed_dim_list:\n",
        "    h.HIDDEN_DIM = k\n",
        "    h.EMBEDDING_DIM = j\n",
        "    print(\"\\n---------------------------------------\")\n",
        "    print(\"training with hidden_dim: \" + str(k) + \", embed_dim: \" + str(j))\n",
        "\n",
        "    name = \"lstm_1layer_base_\" + str(k) + \"_\" + str(j) + \"_e32_h100\"\n",
        "    _ = train_and_test_model_with_hparams(h, name)\n",
        "    test_acc.append(_.get(\"test_acc\"))\n",
        "    max_acc = max(test_acc)\n",
        "    if res != max_acc:\n",
        "      res = max_acc\n",
        "      param = {\"layer\": i, \"hidden\" : k, \"embed\": j}\n",
        "print(max_acc)\n",
        "print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLdjZXKxZ8Ha"
      },
      "source": [
        "### Lab 2 (g) Bi-Directional LSTM, using best architecture from (f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "z9OHVuT0Z8Ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf90edb5-c2da-4be0-f4f9-f8ddb2a7f8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 60794\n",
            "The model has 244,696 trainable parameters\n",
            "training...: 100%|██████████| 365/365 [00:11<00:00, 32.33it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 63.42it/s]\n",
            "Saving ...\n",
            "epoch: 1\n",
            "train_loss: 0.706, train_acc: 0.526\n",
            "valid_loss: 0.680, valid_acc: 0.615\n",
            "training...: 100%|██████████| 365/365 [00:11<00:00, 31.88it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 60.02it/s]\n",
            "Saving ...\n",
            "epoch: 2\n",
            "train_loss: 0.550, train_acc: 0.745\n",
            "valid_loss: 0.385, valid_acc: 0.850\n",
            "training...: 100%|██████████| 365/365 [00:11<00:00, 32.22it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 38.58it/s]\n",
            "Saving ...\n",
            "epoch: 3\n",
            "train_loss: 0.280, train_acc: 0.893\n",
            "valid_loss: 0.339, valid_acc: 0.860\n",
            "training...: 100%|██████████| 365/365 [00:12<00:00, 29.92it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:01<00:00, 41.47it/s]\n",
            "Saving ...\n",
            "epoch: 4\n",
            "train_loss: 0.165, train_acc: 0.944\n",
            "valid_loss: 0.326, valid_acc: 0.880\n",
            "training...: 100%|██████████| 365/365 [00:11<00:00, 31.99it/s]\n",
            "evaluating...: 100%|██████████| 53/53 [00:00<00:00, 66.27it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.103, train_acc: 0.969\n",
            "valid_loss: 0.374, valid_acc: 0.876\n",
            "testing...: 100%|██████████| 105/105 [00:01<00:00, 64.86it/s]\n",
            "test_loss: 0.322, test_acc: 0.876\n"
          ]
        }
      ],
      "source": [
        "h = HyperParams()\n",
        "h.LR = 0.001\n",
        "h.OPTIM = \"adam\"\n",
        "h.N_LAYERS = 1\n",
        "h.HIDDEN_DIM = 150\n",
        "h.EMBEDDING_DIM = 1\n",
        "h.BIDIRECTIONAL = True\n",
        "name = \"lstm_1layer_base_\" + str(k) + \"_\" + str(j) + \"_e32_h100\"\n",
        "_ = train_and_test_model_with_hparams(h, name)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}